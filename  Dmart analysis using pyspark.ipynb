{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03d68f1e-f8f0-494c-9984-dcf674759a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count, avg, round, countDistinct\n",
    "from pyspark.sql.types import IntegerType, FloatType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ae78056-824f-4139-adaf-784f7ecad6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1: Establish PySpark Connection\n",
    "def init_spark():\n",
    "    \"\"\"\n",
    "    Initialize a Spark session for PySpark processing.\n",
    "    Output: SparkSession object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        spark = SparkSession.builder.appName(\"DMartSalesAnalysis\").getOrCreate()\n",
    "        print(\"Spark session initialized successfully.\")\n",
    "        return spark\n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing Spark session: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "351c7b64-2803-46be-ad79-43dfe8b90623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2: Load Data into PySpark DataFrames\n",
    "def load_data(spark, products_file, sales_file, customers_file):\n",
    "    \"\"\"\n",
    "    Load CSV files into PySpark DataFrames.\n",
    "    Input: SparkSession, paths to products.csv, sales.csv, customers.csv\n",
    "    Output: Tuple of DataFrames (products_df, sales_df, customers_df)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        products_df = spark.read.option(\"header\", \"true\").csv(products_file)\n",
    "        print(f\"Loaded products.csv with {products_df.count()} rows.\")\n",
    "        print(\"Products DataFrame schema:\")\n",
    "        products_df.printSchema()\n",
    "        sales_df = spark.read.option(\"header\", \"true\").csv(sales_file)\n",
    "        print(f\"Loaded sales.csv with {sales_df.count()} rows.\")\n",
    "        print(\"Sales DataFrame schema:\")\n",
    "        sales_df.printSchema()\n",
    "        customers_df = spark.read.option(\"header\", \"true\").csv(customers_file)\n",
    "        print(f\"Loaded customers.csv with {customers_df.count()} rows.\")\n",
    "        print(\"Customers DataFrame schema:\")\n",
    "        customers_df.printSchema()\n",
    "        return products_df, sales_df, customers_df\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {str(e)}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "457af322-d9a4-432b-a251-88c7e22174ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(products_df, sales_df, customers_df):\n",
    "    \"\"\"\n",
    "    Clean and transform DataFrames, then join them.\n",
    "    Input: Products, sales, and customers DataFrames\n",
    "    Output: Integrated DataFrame after cleaning and joining\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Rename columns for consistency\n",
    "        products_df = products_df.withColumnRenamed(\"Product ID\", \"product_id\") \\\n",
    "                                 .withColumnRenamed(\"Category\", \"category\") \\\n",
    "                                 .withColumnRenamed(\"Sub-Category\", \"sub_category\") \\\n",
    "                                 .withColumnRenamed(\"Product Name\", \"product_name\")\n",
    "        \n",
    "        sales_df = sales_df.withColumnRenamed(\"Order Line\", \"order_line\") \\\n",
    "                           .withColumnRenamed(\"Order ID\", \"order_id\") \\\n",
    "                           .withColumnRenamed(\"Order Date\", \"order_date\") \\\n",
    "                           .withColumnRenamed(\"Ship Date\", \"ship_date\") \\\n",
    "                           .withColumnRenamed(\"Ship Mode\", \"ship_mode\") \\\n",
    "                           .withColumnRenamed(\"Customer ID\", \"customer_id\") \\\n",
    "                           .withColumnRenamed(\"Product ID\", \"product_id\") \\\n",
    "                           .withColumnRenamed(\"Sales\", \"sales\") \\\n",
    "                           .withColumnRenamed(\"Quantity\", \"quantity\") \\\n",
    "                           .withColumnRenamed(\"Discount\", \"discount\") \\\n",
    "                           .withColumnRenamed(\"Profit\", \"profit\")\n",
    "        \n",
    "        customers_df = customers_df.withColumnRenamed(\"Customer ID\", \"customer_id\") \\\n",
    "                                   .withColumnRenamed(\"Customer Name\", \"customer_name\") \\\n",
    "                                   .withColumnRenamed(\"Segment\", \"customer_segment\") \\\n",
    "                                   .withColumnRenamed(\"Age\", \"age\") \\\n",
    "                                   .withColumnRenamed(\"Country\", \"country\") \\\n",
    "                                   .withColumnRenamed(\"City\", \"city\") \\\n",
    "                                   .withColumnRenamed(\"State\", \"state\") \\\n",
    "                                   .withColumnRenamed(\"Postal Code\", \"postal_code\") \\\n",
    "                                   .withColumnRenamed(\"Region\", \"region\")\n",
    "\n",
    "        # Handle missing values\n",
    "        products_df = products_df.na.fill({\"product_name\": \"Unknown\"})\n",
    "        sales_df = sales_df.na.fill({\"quantity\": 0, \"discount\": 0.0, \"sales\": 0.0, \"profit\": 0.0})\n",
    "        customers_df = customers_df.na.fill({\"age\": 0, \"customer_name\": \"Unknown\"})\n",
    "\n",
    "        # Ensure correct data types\n",
    "        products_df = products_df.withColumn(\"product_id\", col(\"product_id\").cast(IntegerType()))\n",
    "        sales_df = sales_df.withColumn(\"order_line\", col(\"order_line\").cast(IntegerType())) \\\n",
    "                           .withColumn(\"product_id\", col(\"product_id\").cast(IntegerType())) \\\n",
    "                           .withColumn(\"customer_id\", col(\"customer_id\").cast(IntegerType())) \\\n",
    "                           .withColumn(\"quantity\", col(\"quantity\").cast(IntegerType())) \\\n",
    "                           .withColumn(\"discount\", col(\"discount\").cast(FloatType())) \\\n",
    "                           .withColumn(\"sales\", col(\"sales\").cast(FloatType())) \\\n",
    "                           .withColumn(\"profit\", col(\"profit\").cast(FloatType())) \\\n",
    "                           .withColumn(\"order_date\", col(\"order_date\").cast(TimestampType())) \\\n",
    "                           .withColumn(\"ship_date\", col(\"ship_date\").cast(TimestampType()))\n",
    "        customers_df = customers_df.withColumn(\"customer_id\", col(\"customer_id\").cast(IntegerType())) \\\n",
    "                                   .withColumn(\"age\", col(\"age\").cast(IntegerType())) \\\n",
    "                                   .withColumn(\"postal_code\", col(\"postal_code\").cast(IntegerType()))\n",
    "\n",
    "        # Join DataFrames\n",
    "        integrated_df = sales_df.join(products_df.select(\"product_id\", \"category\", \"sub_category\", \"product_name\"), \n",
    "                                     \"product_id\", \"left\") \\\n",
    "                               .join(customers_df.select(\"customer_id\", \"customer_name\", \"customer_segment\", \n",
    "                                                        \"age\", \"country\", \"city\", \"state\", \"postal_code\", \"region\"), \n",
    "                                     \"customer_id\", \"left\")\n",
    "        print(f\"Task 3: Integrated DataFrame created with {integrated_df.count()} rows.\")\n",
    "        return integrated_df\n",
    "    except Exception as e:\n",
    "        print(f\"Task 3: Error transforming data: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "839127c5-8d96-45e1-bbdb-bbe6ac1d9681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session initialized successfully.\n",
      "Loaded products.csv with 1862 rows.\n",
      "Products DataFrame schema:\n",
      "root\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Sub-Category: string (nullable = true)\n",
      " |-- Product Name: string (nullable = true)\n",
      "\n",
      "Loaded sales.csv with 9994 rows.\n",
      "Sales DataFrame schema:\n",
      "root\n",
      " |-- Order Line: string (nullable = true)\n",
      " |-- Order ID: string (nullable = true)\n",
      " |-- Order Date: string (nullable = true)\n",
      " |-- Ship Date: string (nullable = true)\n",
      " |-- Ship Mode: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Product ID: string (nullable = true)\n",
      " |-- Sales: string (nullable = true)\n",
      " |-- Quantity: string (nullable = true)\n",
      " |-- Discount: string (nullable = true)\n",
      " |-- Profit: string (nullable = true)\n",
      "\n",
      "Loaded customers.csv with 793 rows.\n",
      "Customers DataFrame schema:\n",
      "root\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Customer Name: string (nullable = true)\n",
      " |-- Segment: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Postal Code: string (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      "\n",
      "Task 3: Integrated DataFrame created with 9994 rows.\n",
      "Task 4: Formulating analytical questions.\n",
      "\n",
      "--- Analytical Queries ---\n",
      "\n",
      "Running query: Total sales by category\n",
      "1. Total sales for each product category\n",
      "+--------+-----------+\n",
      "|category|total_sales|\n",
      "+--------+-----------+\n",
      "|    NULL| 2297200.86|\n",
      "+--------+-----------+\n",
      "\n",
      "\n",
      "Running query: Top customer by purchases\n",
      "2. Customer with the highest number of purchases\n",
      "+-----------+-------------+--------------+\n",
      "|customer_id|customer_name|purchase_count|\n",
      "+-----------+-------------+--------------+\n",
      "|       NULL|         NULL|          9994|\n",
      "+-----------+-------------+--------------+\n",
      "\n",
      "\n",
      "Running query: Average discount\n",
      "3. Average discount given on sales across all products\n",
      "+------------+\n",
      "|avg_discount|\n",
      "+------------+\n",
      "|      0.1562|\n",
      "+------------+\n",
      "\n",
      "\n",
      "Running query: Unique products by region\n",
      "4. Unique products sold in each region\n",
      "+------+---------------+\n",
      "|region|unique_products|\n",
      "+------+---------------+\n",
      "|  NULL|              0|\n",
      "+------+---------------+\n",
      "\n",
      "\n",
      "Running query: Total profit by state\n",
      "5. Total profit generated in each state\n",
      "+-----+------------+\n",
      "|state|total_profit|\n",
      "+-----+------------+\n",
      "| NULL|   286397.02|\n",
      "+-----+------------+\n",
      "\n",
      "\n",
      "Running query: Top sub-category by sales\n",
      "6. Product sub-category with the highest sales\n",
      "+------------+-----------+\n",
      "|sub_category|total_sales|\n",
      "+------------+-----------+\n",
      "|        NULL| 2297200.86|\n",
      "+------------+-----------+\n",
      "\n",
      "\n",
      "Running query: Average age by segment\n",
      "7. Average age of customers in each segment\n",
      "+----------------+-------+\n",
      "|customer_segment|avg_age|\n",
      "+----------------+-------+\n",
      "|            NULL|   NULL|\n",
      "+----------------+-------+\n",
      "\n",
      "\n",
      "Running query: Orders by shipping mode\n",
      "8. Orders shipped in each shipping mode\n",
      "+--------------+-----------+\n",
      "|     ship_mode|order_count|\n",
      "+--------------+-----------+\n",
      "|Standard Class|       5968|\n",
      "|  Second Class|       1945|\n",
      "|   First Class|       1538|\n",
      "|      Same Day|        543|\n",
      "+--------------+-----------+\n",
      "\n",
      "\n",
      "Running query: Total quantity by city\n",
      "9. Total quantity of products sold in each city\n",
      "+----+--------------+\n",
      "|city|total_quantity|\n",
      "+----+--------------+\n",
      "|NULL|         37873|\n",
      "+----+--------------+\n",
      "\n",
      "\n",
      "Running query: Top segment by profit margin\n",
      "10. Customer segment with the highest profit margin\n",
      "+----------------+-------------+\n",
      "|customer_segment|profit_margin|\n",
      "+----------------+-------------+\n",
      "|            NULL|       0.1203|\n",
      "+----------------+-------------+\n",
      "\n",
      "Task 5: All queries executed successfully.\n",
      "\n",
      "Summary:\n",
      "Total records processed: 9994\n",
      "Total sales across all records: $2297200.86\n",
      "Total profit across all records: $286397.02\n",
      "Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "def define_queries():\n",
    "    \"\"\"\n",
    "    Define the 10 analytical questions as a list of query functions.\n",
    "    Output: List of tuples (query_name, query_function)\n",
    "    \"\"\"\n",
    "    print(\"Task 4: Formulating analytical questions.\")\n",
    "    \n",
    "    queries = [\n",
    "        (\"Total sales by category\", query_1),\n",
    "        (\"Top customer by purchases\", query_2),\n",
    "        (\"Average discount\", query_3),\n",
    "        (\"Unique products by region\", query_4),\n",
    "        (\"Total profit by state\", query_5),\n",
    "        (\"Top sub-category by sales\", query_6),\n",
    "        (\"Average age by segment\", query_7),\n",
    "        (\"Orders by shipping mode\", query_8),\n",
    "        (\"Total quantity by city\", query_9),\n",
    "        (\"Top segment by profit margin\", query_10)\n",
    "    ]\n",
    "    return queries\n",
    "\n",
    "# ------------------- Task 5: Execute Analytical Queries -------------------\n",
    "\n",
    "def run_queries(integrated_df, queries):\n",
    "    \"\"\"\n",
    "    Execute the defined analytical queries.\n",
    "    Input: Integrated DataFrame, list of (query_name, query_function)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\n--- Analytical Queries ---\")\n",
    "        for query_name, query_func in queries:\n",
    "            print(f\"\\nRunning query: {query_name}\")\n",
    "            query_func(integrated_df)\n",
    "        print(\"Task 5: All queries executed successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Task 5: Error executing queries: {str(e)}\")\n",
    "\n",
    "# ------------------- Main Execution -------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the PySpark pipeline and analysis.\n",
    "    \"\"\"\n",
    "    # File paths\n",
    "    products_file = \"file:///C:/Guvi/Project_5/Product.csv\"\n",
    "    sales_file = \"file:///C:/Guvi/Project_5/Sales.csv\"\n",
    "    customers_file = \"file:///C:/Guvi/Project_5/Customer.csv\"\n",
    "\n",
    "    # Verify file existence\n",
    "    for file_path in [products_file.replace(\"file:///\", \"\"), sales_file.replace(\"file:///\", \"\"), customers_file.replace(\"file:///\", \"\")]:\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Error: File not found at {file_path}\")\n",
    "            return\n",
    "\n",
    "    # Task 1: Initialize Spark\n",
    "    spark = init_spark()\n",
    "    if not spark:\n",
    "        return\n",
    "\n",
    "    # Task 2: Load data\n",
    "    products_df, sales_df, customers_df = load_data(spark, products_file, sales_file, customers_file)\n",
    "    if not all([products_df, sales_df, customers_df]):\n",
    "        spark.stop()\n",
    "        return\n",
    "\n",
    "    # Task 3: Transform and clean data\n",
    "    integrated_df = transform_data(products_df, sales_df, customers_df)\n",
    "    if not integrated_df:\n",
    "        spark.stop()\n",
    "        return\n",
    "\n",
    "    # Task 4: Define queries\n",
    "    queries = define_queries()\n",
    "\n",
    "    # Task 5: Run queries\n",
    "    run_queries(integrated_df, queries)\n",
    "\n",
    "    # Summary statistics\n",
    "    try:\n",
    "        total_sales = integrated_df.agg(round(sum(\"sales\"), 2).alias(\"total_sales\")).collect()[0][\"total_sales\"]\n",
    "        total_profit = integrated_df.agg(round(sum(\"profit\"), 2).alias(\"total_profit\")).collect()[0][\"total_profit\"]\n",
    "        total_rows = integrated_df.count()\n",
    "        print(\"\\nSummary:\")\n",
    "        print(f\"Total records processed: {total_rows}\")\n",
    "        print(f\"Total sales across all records: ${total_sales}\")\n",
    "        print(f\"Total profit across all records: ${total_profit}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating summary: {str(e)}\")\n",
    "\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n",
    "    print(\"Spark session stopped.\")\n",
    "\n",
    "# ------------------- Analytical Questions Code -------------------\n",
    "\n",
    "\"\"\"\n",
    "Analytical Questions Code\n",
    "The following functions define the logic for the 10 analytical questions.\n",
    "They are used in Task 4 (define_queries) and executed in Task 5 (run_queries).\n",
    "\"\"\"\n",
    "\n",
    "def query_1(df):\n",
    "    print(\"1. Total sales for each product category\")\n",
    "    df.groupBy(\"category\") \\\n",
    "      .agg(round(sum(\"sales\"), 2).alias(\"total_sales\")) \\\n",
    "      .orderBy(\"total_sales\", ascending=False) \\\n",
    "      .show()\n",
    "\n",
    "def query_2(df):\n",
    "    print(\"2. Customer with the highest number of purchases\")\n",
    "    df.groupBy(\"customer_id\", \"customer_name\") \\\n",
    "      .agg(count(\"order_line\").alias(\"purchase_count\")) \\\n",
    "      .orderBy(\"purchase_count\", ascending=False) \\\n",
    "      .limit(1) \\\n",
    "      .show()\n",
    "\n",
    "def query_3(df):\n",
    "    print(\"3. Average discount given on sales across all products\")\n",
    "    df.agg(round(avg(\"discount\"), 4).alias(\"avg_discount\")) \\\n",
    "      .show()\n",
    "\n",
    "def query_4(df):\n",
    "    print(\"4. Unique products sold in each region\")\n",
    "    df.groupBy(\"region\") \\\n",
    "      .agg(countDistinct(\"product_id\").alias(\"unique_products\")) \\\n",
    "      .orderBy(\"region\") \\\n",
    "      .show()\n",
    "\n",
    "def query_5(df):\n",
    "    print(\"5. Total profit generated in each state\")\n",
    "    df.groupBy(\"state\") \\\n",
    "      .agg(round(sum(\"profit\"), 2).alias(\"total_profit\")) \\\n",
    "      .orderBy(\"total_profit\", ascending=False) \\\n",
    "      .show()\n",
    "\n",
    "def query_6(df):\n",
    "    print(\"6. Product sub-category with the highest sales\")\n",
    "    df.groupBy(\"sub_category\") \\\n",
    "      .agg(round(sum(\"sales\"), 2).alias(\"total_sales\")) \\\n",
    "      .orderBy(\"total_sales\", ascending=False) \\\n",
    "      .limit(1) \\\n",
    "      .show()\n",
    "\n",
    "def query_7(df):\n",
    "    print(\"7. Average age of customers in each segment\")\n",
    "    df.groupBy(\"customer_segment\") \\\n",
    "      .agg(round(avg(\"age\"), 2).alias(\"avg_age\")) \\\n",
    "      .orderBy(\"customer_segment\") \\\n",
    "      .show()\n",
    "\n",
    "def query_8(df):\n",
    "    print(\"8. Orders shipped in each shipping mode\")\n",
    "    df.groupBy(\"ship_mode\") \\\n",
    "      .agg(count(\"order_line\").alias(\"order_count\")) \\\n",
    "      .orderBy(\"order_count\", ascending=False) \\\n",
    "      .show()\n",
    "\n",
    "def query_9(df):\n",
    "    print(\"9. Total quantity of products sold in each city\")\n",
    "    df.groupBy(\"city\") \\\n",
    "      .agg(sum(\"quantity\").alias(\"total_quantity\")) \\\n",
    "      .orderBy(\"total_quantity\", ascending=False) \\\n",
    "      .show()\n",
    "\n",
    "def query_10(df):\n",
    "    print(\"10. Customer segment with the highest profit margin\")\n",
    "    df.groupBy(\"customer_segment\") \\\n",
    "      .agg(round(avg(col(\"profit\") / col(\"sales\")), 4).alias(\"profit_margin\")) \\\n",
    "      .orderBy(\"profit_margin\", ascending=False) \\\n",
    "      .limit(1) \\\n",
    "      .show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba24604-2d3d-4811-96bb-b9afbd7a574b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
